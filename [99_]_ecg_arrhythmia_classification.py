# -*- coding: utf-8 -*-
"""[99%] ECG Arrhythmia Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/thesubham20000/99-ecg-arrhythmia-classification.60c5d499-486c-42d6-9981-743b5f897919.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250731/auto/storage/goog4_request%26X-Goog-Date%3D20250731T171715Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0252d8825548dafb58ae00caa9d7f1a43a28adaa5e99011011ce9e9eeb8f033b043fad788add7175edb155f9405d0eb42caf69bf3ef09b20028264c477e55d6d97310b019ea383d74d17fa7e700699bec6fd11facfeb969433ca46721e612d92e630223c025c5a9ca064586a7336d2ee6d3452f77a13313a3c87b0471ddb3e73edbd23b4ed85cd138c260ba477e24945e56a015fe2497b7a33e6856e5d02fd48f66459c8905f2829d5c9a4ca6ccf7b5f23165e333cf14d8161c6794ccb07dd799676c5994b8ab8b79892d842e3a9b0369f2a982d690ef1a27701638899d401be203bcbd72b81f30b659474659903aa9c73bd9ca9edcc688409cbaa1078ef105e
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
sadmansakib7_ecg_arrhythmia_classification_dataset_path = kagglehub.dataset_download('sadmansakib7/ecg-arrhythmia-classification-dataset')

print('Data source import complete.')

"""# <span style="display: block; text-align: center; color: gold;">Import Libraries</span>

"""

import optuna
import pandas as pd

import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

"""# <span style="display: block; text-align: center; color: gold;">Preliminary Dataset Loading</span>

"""

df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

df.head()

"""# <span style="display: block; text-align: center; color: gold;">Exploratory Data Analysis</span>

"""

df.info()

print(f'Dataset Shape: {df.shape}\n')

df.describe()

print(df.isnull().sum())

print(df['type'].value_counts())

df['type'].value_counts().plot(kind='bar', title='Class Distribution')
plt.show()

"""# <span style="display: block; text-align: center; color: gold;">Data Prepocessing</span>

"""

label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])

print(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

X = df.drop('type', axis=1)
y = df['type']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

"""# <span style="display: block; text-align: center; color: gold;">Baseline Models</span>

"""

from imblearn.over_sampling import ADASYN

# Instantiate ADASYN
adasyn = ADASYN(random_state=42)

# Fit and resample
X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)

import matplotlib.pyplot as plt

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
y_train.value_counts().plot(kind='bar', title='Before ADASYN')
plt.subplot(1,2,2)
pd.Series(y_train_resampled).value_counts().plot(kind='bar', title='After ADASYN')
plt.tight_layout()
plt.show()

"""## <span style="display: block; text-align: center; color: aqua;">XGBoost</span>

"""

# from xgboost import XGBClassifier
# from sklearn.metrics import classification_report
# import numpy as np

# xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', verbosity=0, random_state=42)

# xgb_model.fit(X_train, y_train)

# y_pred_xgb = xgb_model.predict(X_val)

# print("XGBoost Classification Report:")

# # FIX: Use .tolist() to avoid TypeError
# # print(classification_report(y_val, y_pred_xgb, target_names=label_encoder.classes_.tolist()))

"""## <span style="display: block; text-align: center; color: aqua;">CatBoost</span>

"""

# cat_model = CatBoostClassifier(verbose=0, random_state=42)

# cat_model.fit(X_train, y_train)

# y_pred_cat = cat_model.predict(X_val)

# print("CatBoost Classification Report:")
# print(classification_report(y_val, y_pred_cat, target_names=label_encoder.classes_))

"""## <span style="display: block; text-align: center; color: aqua;">LightGBM</span>

"""

# lgbm_model = LGBMClassifier(verbosity=-1, random_state=42)

# lgbm_model.fit(X_train, y_train)

# y_pred_lgbm = lgbm_model.predict(X_val)

# print("LightGBM Classification Report:")
# print(classification_report(y_val, y_pred_lgbm, target_names=label_encoder.classes_))
# #

# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.metrics import classification_report

# # Initialize AdaBoost Classifier
# ada_model = AdaBoostClassifier(random_state=42)

# # Fit model on training data
# ada_model.fit(X_train, y_train)

# # Predict on validation data
# y_pred_ada = ada_model.predict(X_val)

# # Print classification report
# print("AdaBoost Classification Report:")
# print(classification_report(y_val, y_pred_ada, target_names=label_encoder.classes_))

from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    AdaBoostClassifier,
    BaggingClassifier,
    VotingClassifier,
    ExtraTreesClassifier
)


knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_val)

print("K-Nearest Neighbors Classification Report:")
print(classification_report(y_val, y_pred_knn, target_names=label_encoder.classes_))

svm_model = SVC(kernel='rbf', random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_val)

print("Kernel SVM Classification Report:")
print(classification_report(y_val, y_pred_svm, target_names=label_encoder.classes_))

nb_model = GaussianNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_val)

print("Naive Bayes Classification Report:")
print(classification_report(y_val, y_pred_nb, target_names=label_encoder.classes_))

# ðŸ“š Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ðŸ“‚ Load Dataset
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# ðŸŽ¯ Encode Labels
label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])

# ðŸŽ¯ Features and Target
X = df.drop('type', axis=1)
y = df['type']

# ðŸ“ˆ Split into Train, Validation, and Test (Same as Before)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# ðŸ§  Train Classifiers Directly on Imbalanced Data

# 1ï¸âƒ£ XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_val)

print("\nðŸ”¹ XGBoost (Original Imbalanced Data) Classification Report")
print(classification_report(y_val, y_pred_xgb, target_names=label_encoder.classes_.tolist()))

cm_xgb = confusion_matrix(y_val, y_pred_xgb)
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=label_encoder.classes_)
disp_xgb.plot(cmap='Blues')
plt.title('XGBoost Confusion Matrix (Imbalanced)')
plt.show()

# 2ï¸âƒ£ LightGBM
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train, y_train)
y_pred_lgbm = lgbm_model.predict(X_val)

print("\nðŸ”¹ LightGBM (Original Imbalanced Data) Classification Report")
print(classification_report(y_val, y_pred_lgbm, target_names=label_encoder.classes_.tolist()))

cm_lgbm = confusion_matrix(y_val, y_pred_lgbm)
disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm, display_labels=label_encoder.classes_)
disp_lgbm.plot(cmap='Purples')
plt.title('LightGBM Confusion Matrix (Imbalanced)')
plt.show()

# 3ï¸âƒ£ CatBoost
catboost_model = CatBoostClassifier(verbose=0, random_state=42)
catboost_model.fit(X_train, y_train)
y_pred_cat = catboost_model.predict(X_val)

print("\nðŸ”¹ CatBoost (Original Imbalanced Data) Classification Report")
print(classification_report(y_val, y_pred_cat, target_names=label_encoder.classes_.tolist()))

cm_cat = confusion_matrix(y_val, y_pred_cat)
disp_cat = ConfusionMatrixDisplay(confusion_matrix=cm_cat, display_labels=label_encoder.classes_)
disp_cat.plot(cmap='Greens')
plt.title('CatBoost Confusion Matrix (Imbalanced)')
plt.show()

"""Using Smote + TabNet ****"""

# ðŸ“š Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from imblearn.over_sampling import SMOTE

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ðŸ“‚ Load the Dataset
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# ðŸ” Explore the Dataset
print(f'Dataset Shape: {df.shape}\n')
print(df.describe())
print(df['type'].value_counts())

# ðŸ“Š Plot Class Distribution
df['type'].value_counts().plot(kind='bar', title='Class Distribution')
plt.show()

# ðŸŽ¯ Encode the Target Labels
label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])
print("Class Mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

# ðŸŽ¯ Define Features and Target
X = df.drop('type', axis=1)
y = df['type']

# ðŸ“ˆ Train/Validation/Test Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# âš¡ Apply SMOTE to Training Data Only
print("\nBefore SMOTE:", np.bincount(y_train))
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
print("After SMOTE:", np.bincount(y_train_smote))

# ðŸ“š Train Classifiers on SMOTE-balanced Data

# 1ï¸âƒ£ XGBoost Classifier
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_smote, y_train_smote)
y_pred_xgb = xgb_model.predict(X_val)

print("\nXGBoost + SMOTE Classification Report")
print(classification_report(y_val, y_pred_xgb, target_names=label_encoder.classes_.tolist()))

cm_xgb = confusion_matrix(y_val, y_pred_xgb)
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=label_encoder.classes_)
disp_xgb.plot()
plt.title('XGBoost + SMOTE Confusion Matrix')
plt.show()

# 2ï¸âƒ£ LightGBM Classifier
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train_smote, y_train_smote)
y_pred_lgbm = lgbm_model.predict(X_val)

print("\nLightGBM + SMOTE Classification Report")
print(classification_report(y_val, y_pred_lgbm, target_names=label_encoder.classes_.tolist()))

cm_lgbm = confusion_matrix(y_val, y_pred_lgbm)
disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm, display_labels=label_encoder.classes_)
disp_lgbm.plot()
plt.title('LightGBM + SMOTE Confusion Matrix')
plt.show()

# 3ï¸âƒ£ CatBoost Classifier
catboost_model = CatBoostClassifier(verbose=0, random_state=42)
catboost_model.fit(X_train_smote, y_train_smote)
y_pred_cat = catboost_model.predict(X_val)

print("\nCatBoost + SMOTE Classification Report")
print(classification_report(y_val, y_pred_cat, target_names=label_encoder.classes_.tolist()))

cm_cat = confusion_matrix(y_val, y_pred_cat)
disp_cat = ConfusionMatrixDisplay(confusion_matrix=cm_cat, display_labels=label_encoder.classes_)
disp_cat.plot()
plt.title('CatBoost + SMOTE Confusion Matrix')
plt.show()

# ðŸ“š Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from imblearn.over_sampling import ADASYN

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

# ðŸ“‚ Load Dataset
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# ðŸ” Explore Dataset
print(f'Dataset Shape: {df.shape}\n')
print(df.describe())
print(df['type'].value_counts())

# ðŸ“Š Plot Original Class Distribution
df['type'].value_counts().plot(kind='bar', title='Original Class Distribution')
plt.show()

# ðŸŽ¯ Encode Labels
label_encoder = LabelEncoder()
df['type'] = label_encoder.fit_transform(df['type'])
print("Class Mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

# ðŸŽ¯ Define Features and Target
X = df.drop('type', axis=1)
y = df['type']

# ðŸ“ˆ Train/Validation/Test Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# âš¡ Apply ADASYN Directly
adasyn = ADASYN(random_state=42)
X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)

print("\nAfter ADASYN Class Distribution:")
print(pd.Series(y_train_adasyn).value_counts())

# ðŸ“Š Plot New Class Distribution
plt.figure(figsize=(8,5))
pd.Series(y_train_adasyn).value_counts().sort_index().plot(kind='bar', title='Class Distribution After ADASYN')
plt.xticks(ticks=np.arange(len(label_encoder.classes_)), labels=label_encoder.classes_, rotation=45)
plt.show()

# ðŸ“š Train Classifiers on ADASYN Data

# 1ï¸âƒ£ XGBoost
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train_adasyn, y_train_adasyn)
y_pred_xgb = xgb_model.predict(X_val)

print("\nðŸ”¹ XGBoost + ADASYN Data Classification Report")
print(classification_report(y_val, y_pred_xgb, target_names=label_encoder.classes_.tolist()))

cm_xgb = confusion_matrix(y_val, y_pred_xgb)
disp_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_xgb, display_labels=label_encoder.classes_)
disp_xgb.plot(cmap='Blues')
plt.title('XGBoost Confusion Matrix (ADASYN)')
plt.show()

# 2ï¸âƒ£ LightGBM
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train_adasyn, y_train_adasyn)
y_pred_lgbm = lgbm_model.predict(X_val)

print("\nðŸ”¹ LightGBM + ADASYN Data Classification Report")
print(classification_report(y_val, y_pred_lgbm, target_names=label_encoder.classes_.tolist()))

cm_lgbm = confusion_matrix(y_val, y_pred_lgbm)
disp_lgbm = ConfusionMatrixDisplay(confusion_matrix=cm_lgbm, display_labels=label_encoder.classes_)
disp_lgbm.plot(cmap='Purples')
plt.title('LightGBM Confusion Matrix (ADASYN)')
plt.show()

# 3ï¸âƒ£ CatBoost
catboost_model = CatBoostClassifier(verbose=0, random_state=42)
catboost_model.fit(X_train_adasyn, y_train_adasyn)
y_pred_cat = catboost_model.predict(X_val)

print("\nðŸ”¹ CatBoost + ADASYN Data Classification Report")
print(classification_report(y_val, y_pred_cat, target_names=label_encoder.classes_.tolist()))

cm_cat = confusion_matrix(y_val, y_pred_cat)
disp_cat = ConfusionMatrixDisplay(confusion_matrix=cm_cat, display_labels=label_encoder.classes_)
disp_cat.plot(cmap='Greens')
plt.title('CatBoost Confusion Matrix (ADASYN)')
plt.show()

"""**visualization******"""

# Step 1: Load the dataset and inspect
import pandas as pd

# Load dataset
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# Display basic information
df.info(), df.head(), df['type'].value_counts()

# Step 2: Pick 5 random samples from each minority class
import numpy as np

# Setting seed for reproducibility
np.random.seed(42)

# Filter minority classes
minority_classes = ['F', 'SVEB', 'VEB']
sampled_data = {}

# Picking 5 samples from each minority class
for label in minority_classes:
    sampled_data[label] = df[df['type'] == label].sample(5)

# Create a combined dataframe for visualization
sampled_df = pd.concat(sampled_data.values(), keys=sampled_data.keys())

sampled_df

# # Step 3: Visualize samples before SMOTE (Using PCA for 2D visualization)
# from sklearn.decomposition import PCA
# import matplotlib.pyplot as plt
# import seaborn as sns

# # Prepare features and labels
# X_sampled = sampled_df.drop(['record', 'type'], axis=1)
# y_sampled = sampled_df['type']

# # Reduce to 2D using PCA
# pca = PCA(n_components=2)
# X_pca = pca.fit_transform(X_sampled)

# # Create a DataFrame for plotting
# pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])
# pca_df['Class'] = y_sampled.values

# # Plot
# plt.figure(figsize=(10, 6))
# sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Class', s=100, palette='deep')
# plt.title('Samples Before SMOTE (PCA Projection)')
# plt.xlabel('Principal Component 1')
# plt.ylabel('Principal Component 2')
# plt.grid(True)
# plt.legend(title='Class')
# plt.show()

# # Step 4: Apply SMOTE
# from imblearn.over_sampling import SMOTE

# # Instantiate SMOTE
# smote = SMOTE(sampling_strategy='not majority', random_state=42)

# # Apply SMOTE on our small sample set
# X_resampled, y_resampled = smote.fit_resample(X_sampled, y_sampled)

# # Reduce to 2D using PCA for new data
# X_resampled_pca = pca.transform(X_resampled)

# # Create a DataFrame for plotting after SMOTE
# pca_resampled_df = pd.DataFrame(data=X_resampled_pca, columns=['PC1', 'PC2'])
# pca_resampled_df['Class'] = y_resampled

# # Plot before and after SMOTE side-by-side
# fig, axes = plt.subplots(1, 2, figsize=(18, 7))

# # Before SMOTE
# sns.scatterplot(data=pca_df, x='PC1', y='PC2', hue='Class', s=100, palette='deep', ax=axes[0])
# axes[0].set_title('Before SMOTE')
# axes[0].grid(True)

# # After SMOTE
# sns.scatterplot(data=pca_resampled_df, x='PC1', y='PC2', hue='Class', s=100, palette='deep', ax=axes[1], marker='X')
# axes[1].set_title('After SMOTE')
# axes[1].grid(True)

# plt.suptitle('Effect of SMOTE on Minority Classes (PCA Projection)', fontsize=16)
# plt.show()

# # Show how many samples after SMOTE
# pd.Series(y_resampled).value_counts()

"""**PLOT SHOWING SMOTE AND 3 GENERATED SAMPLE**"""

# ðŸ“¦ Step 1: Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

# ðŸ“š Step 2: Load the MIT-BIH dataset
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# Only 34 features + 'type'
X = df.drop(['record', 'type'], axis=1)
y = df['type']

# ðŸ“Š Step 3: Focus on 4 Minor Classes
minor_classes = ['F', 'Q', 'SVEB', 'VEB']

# Pick 5 samples safely
minor_samples = []
for cls in minor_classes:
    available_samples = df[df['type'] == cls]
    if len(available_samples) >= 5:
        samples = available_samples.sample(5, random_state=42)
        minor_samples.append(samples)
    else:
        print(f"Class {cls} has only {len(available_samples)} samples, not enough for 5 samples.")

# Combine into one DataFrame
minor_samples_df = pd.concat(minor_samples).reset_index(drop=True)

# Separate features and labels
X_minor = minor_samples_df.drop(['record', 'type'], axis=1)
y_minor = minor_samples_df['type']

# ðŸ§ª Step 4: Create synthetic samples manually (SMOTE concept)

# Helper function
def create_synthetic_samples(X_class, n_samples=3):
    neighbors = NearestNeighbors(n_neighbors=2).fit(X_class)
    synthetic_samples = []
    for _ in range(n_samples):
        idx = np.random.randint(0, len(X_class))
        sample = X_class.iloc[idx]
        _, indices = neighbors.kneighbors([sample])
        neighbor = X_class.iloc[indices[0][1]]  # pick nearest neighbor
        # Î» random number between 0 and 1
        lam = np.random.uniform(0, 1)
        synthetic = sample + lam * (neighbor - sample)
        synthetic_samples.append(synthetic)
    return pd.DataFrame(synthetic_samples)

# Generate synthetic samples
synthetic_data = []
synthetic_labels = []

for cls in minor_classes:
    X_cls = minor_samples_df[minor_samples_df['type'] == cls].drop(['record', 'type'], axis=1).reset_index(drop=True)
    synth = create_synthetic_samples(X_cls, n_samples=3)
    synthetic_data.append(synth)
    synthetic_labels += [cls] * 3

# Combine synthetic samples
synthetic_data_df = pd.concat(synthetic_data).reset_index(drop=True)

# ðŸ“ˆ Step 5: Combine Real + Synthetic data
X_combined = pd.concat([X_minor, synthetic_data_df]).reset_index(drop=True)
y_combined = pd.concat([y_minor, pd.Series(synthetic_labels)]).reset_index(drop=True)

# âš¡ Step 6: PCA for visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_combined)

# ðŸŽ¨ Step 7: Plotting
plt.figure(figsize=(12, 8))

# Plot original points
sns.scatterplot(x=X_pca[:20, 0], y=X_pca[:20, 1],
                hue=y_combined[:20],
                style=["Original"]*20,
                palette='deep',
                s=100,
                legend="full")

# Plot synthetic points
sns.scatterplot(x=X_pca[20:, 0], y=X_pca[20:, 1],
                hue=y_combined[20:],
                style=["Synthetic"]*12,
                palette='deep',
                markers="X",
                s=200,
                legend=False)

plt.title('Original and Synthetic Samples (PCA projection)', fontsize=16)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.legend(title='Class', bbox_to_anchor=(1.05, 1), loc=2)
plt.show()

# Output final shapes
print(f"Original Minor Samples Shape: {X_minor.shape}")
print(f"Synthetic Samples Shape: {synthetic_data_df.shape}")

"""**PLOT SHOWING ADASYN AND 3 GENERATED SAMPLE**"""

# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors

# Step 1: Load data
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# Step 2: Prepare features and labels
X = df.drop(['record', 'type'], axis=1)
y = df['type']

minor_classes = ['F', 'Q', 'SVEB', 'VEB']

# Step 3: Pick 5 samples from each minor class
minor_samples = []
for cls in minor_classes:
    samples = df[df['type'] == cls].sample(5, random_state=42)
    minor_samples.append(samples)

minor_samples_df = pd.concat(minor_samples).reset_index(drop=True)

X_minor = minor_samples_df.drop(['record', 'type'], axis=1)
y_minor = minor_samples_df['type']

# Step 4: Simulate ADASYN
def create_adasyn_samples(X_class, n_samples=3):
    neighbors = NearestNeighbors(n_neighbors=5).fit(X_class)
    synthetic_samples = []
    for _ in range(n_samples):
        idx = np.random.randint(0, len(X_class))
        sample = X_class.iloc[idx]
        dists, indices = neighbors.kneighbors([sample])

        # Choose neighbor based on distance (harder examples = closer neighbors)
        probs = dists.flatten()
        probs = probs / probs.sum()  # normalize to make probability distribution
        neighbor_idx = np.random.choice(indices.flatten(), p=probs)
        neighbor = X_class.iloc[neighbor_idx]

        # Create synthetic sample
        lam = np.random.uniform(0, 1)
        synthetic = sample + lam * (neighbor - sample)
        synthetic_samples.append(synthetic)
    return pd.DataFrame(synthetic_samples)

# Generate synthetic samples
synthetic_data = []
synthetic_labels = []

for cls in minor_classes:
    X_cls = minor_samples_df[minor_samples_df['type'] == cls].drop(['record', 'type'], axis=1).reset_index(drop=True)
    synth = create_adasyn_samples(X_cls, n_samples=3)
    synthetic_data.append(synth)
    synthetic_labels += [cls] * 3

synthetic_data_df = pd.concat(synthetic_data).reset_index(drop=True)

# Step 5: Combine real and synthetic
X_combined = pd.concat([X_minor, synthetic_data_df]).reset_index(drop=True)
y_combined = pd.concat([y_minor, pd.Series(synthetic_labels)]).reset_index(drop=True)

# Step 6: PCA for visualization
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_combined)

# Step 7: Plot
plt.figure(figsize=(12, 8))

# Plot original points
sns.scatterplot(x=X_pca[:20, 0], y=X_pca[:20, 1], hue=y_combined[:20], style=["Original"]*20, palette='deep', s=100, legend="full")
# Plot synthetic points
sns.scatterplot(x=X_pca[20:, 0], y=X_pca[20:, 1], hue=y_combined[20:], style=["Synthetic"]*12, palette='deep', markers="X", s=200, legend=False)

plt.title('ADASYN: Original and Synthetic Samples (PCA projection)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.legend(title='Class')
plt.show()

print("Original Minor Samples Shape:", X_minor.shape)
print("Synthetic Samples Shape:", synthetic_data_df.shape)

"""**SHOWING 10 PLOTS GETTING GENERATED ON THE GRAPH USING PCA**"""

# Let's code it out to visualize and analyze the PCA result for the 10 samples.

# Imports
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Step 1: Load data
df = pd.read_csv('/kaggle/input/ecg-arrhythmia-classification-dataset/MIT-BIH Arrhythmia Database.csv')

# Step 2: Prepare features
X = df.drop(['record', 'type'], axis=1)

# Step 3: Take first 10 samples
X_10 = X.iloc[:10]

# Step 4: Apply PCA
pca = PCA(n_components=2, random_state=42)
X_2D = pca.fit_transform(X_10)

# Step 5: Plot
plt.figure(figsize=(8,6))
plt.scatter(X_2D[:, 0], X_2D[:, 1], c='blue', edgecolor='k', s=100)
for i in range(len(X_2D)):
    plt.text(X_2D[i, 0]+0.02, X_2D[i, 1]+0.02, str(i), fontsize=9)

plt.title('PCA Projection of 10 ECG Samples (2D)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

# Display explained variance
pca.explained_variance_ratio_, pca.explained_variance_ratio_.sum()

"""## <span style="display: block; text-align: center; color: aqua;">Soft Ensemble</span>

"""

# # Step 1: Load the dataset and inspect
# import pandas as pd

# # Load dataset
# df = pd.read_csv('/mnt/data/MIT-BIH Arrhythmia Database.csv')

# # Display basic information
# df.info(), df.head(), df['type'].value_counts()

# ensemble_model = VotingClassifier(
#     estimators=[
#         ('xgb', xgb_model),
#         ('cat', cat_model),
#         ('lgbm', lgbm_model)
#     # ],
#     voting='soft'
# )

# ensemble_model.fit(X_train, y_train)

# y_pred_ensemble = ensemble_model.predict(X_val)

# print("Ensemble Model Classification Report:")
# print(classification_report(y_val, y_pred_ensemble, target_names=label_encoder.classes_))

# cm = confusion_matrix(y_val, y_pred_ensemble)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
# disp.plot()
# plt.title("Ensemble Model Confusion Matrix")
# plt.show()

# y_pred_ensemble_test = ensemble_model.predict(X_test)

# print("Ensemble Model (Test Set) Classification Report:")
# print(classification_report(y_test, y_pred_ensemble_test, target_names=label_encoder.classes_))

# cm = confusion_matrix(y_test, y_pred_ensemble_test)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
# disp.plot()
# plt.title("Ensemble Model Confusion Matrix (Test Set)")
# plt.show()

"""# <span style="display: block; text-align: center; color: gold;">Parameter Search</span>

"""

# def objective(trial):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 100, 400),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'gamma': trial.suggest_float('gamma', 0, 0.3),
#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 5)
#     }

#     model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', **params, random_state=42)
#     scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro')
#     return scores.mean()

# study = optuna.create_study(direction='maximize')
# study.optimize(objective, n_trials=50, n_jobs=-1)

# print("Best Hyperparameters for XGBoost (Optuna):")
# print(study.best_params)

# best_xgb_model = XGBClassifier(
#     use_label_encoder=False,
#     eval_metric='mlogloss',
#     **study.best_params,
#     random_state=42
# )
# best_xgb_model.fit(X_train, y_train)

# y_pred_xgb_optuna = best_xgb_model.predict(X_val)

# print("Optuna-Tuned XGBoost Classification Report:")
# print(classification_report(y_val, y_pred_xgb_optuna, target_names=label_encoder.classes_))

# cm = confusion_matrix(y_val, y_pred_xgb_optuna)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
# disp.plot()
# plt.title("Optuna-Tuned XGBoost Confusion Matrix")
# plt.show()

# y_pred_xgb_test_optuna = best_xgb_model.predict(X_test)

# print("Optuna-Tuned XGBoost (Test Set) Classification Report:")
# print(classification_report(y_test, y_pred_xgb_test_optuna, target_names=label_encoder.classes_))

# cm = confusion_matrix(y_test, y_pred_xgb_test_optuna)
# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)
# disp.plot()
# plt.title("Optuna-Tuned XGBoost Confusion Matrix (Test Set)")
# plt.show()

# importances = best_xgb_model.feature_importances_
# plt.figure(figsize=(10, 8))
# plt.barh(X.columns, importances)
# plt.title("Optuna-Tuned XGBoost Feature Importance")
# plt.xlabel("Feature Importance Score")
# plt.show()

